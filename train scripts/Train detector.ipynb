{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf6-2Rcd8abq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "BATCH_SIZE = 4 # increase / decrease according to GPU memeory\n",
        "RESIZE_TO = 512 # resize the image for training and transforms\n",
        "NUM_EPOCHS = 10 # number of epochs to train for\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "image_dir = 'drive/MyDrive/DetectionDataset/JPEGImages'\n",
        "annotation_dir = 'drive/MyDrive/DetectionDataset/Annotations'\n",
        "\n",
        "classes = [\n",
        "    'background', 'fore'\n",
        "]\n",
        "\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwi2MOJB9QL5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UYQ-5CQ82fI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob as glob\n",
        "from xml.etree import ElementTree as et\n",
        "#from config import CLASSES, RESIZE_TO, TRAIN_DIR, VALID_DIR, BATCH_SIZE\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from utils import collate_fn, get_train_transform, get_valid_transform\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xgmpiZVSAZf"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, Resize, ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3wexZrLUjUr"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations==0.4.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFgUq7S_Unse"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7F_36YkRuVj"
      },
      "outputs": [],
      "source": [
        "def get_transform():\n",
        "  return A.Compose(\n",
        "        [\n",
        "         A.Flip(0.5),\n",
        "         ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        bbox_params={\n",
        "        'format': 'pascal_voc',\n",
        "        'label_fields': ['labels']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r71yxI3n5ffL"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number \n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3CMCmyRQriY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M3VLNBN83H8"
      },
      "outputs": [],
      "source": [
        "class AcneDetectDataset(Dataset):\n",
        "    def __init__(self, dir_path_image, dir_path_ann, width, height, classes, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.dir_path_image = dir_path_image\n",
        "        self.dir_path_ann = dir_path_ann\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.classes = classes\n",
        "\n",
        "        self.image_paths = glob.glob(f\"{self.dir_path_image}/*.jpg\")\n",
        "        self.annotayions = glob.glob(f\"{self.dir_path_ann}/*.xml\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # capture the image name and the full image path\n",
        "        image_name = self.image_paths[idx].split('/')[-1]\n",
        "        image_path = self.image_paths[idx]\n",
        "        # read the image\n",
        "        image = cv2.imread(image_path)\n",
        "        image_target = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image_target = cv2.resize(image_target, (self.width, self.height))\n",
        "        image_target /= 255.0\n",
        "        \n",
        "        # capture the corresponding XML file for getting the annotations\n",
        "        annot_filename = image_name[:-4] + '.xml'\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        tree = et.parse(self.dir_path_ann + '/' + annot_filename)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        # get the height and width of the image\n",
        "        image_width = image.shape[1]\n",
        "        image_height = image.shape[0]\n",
        "\n",
        "        for member in root.findall('object'):\n",
        "            # map the current object name to `classes` list to get...\n",
        "            # ... the label index and append to `labels` list\n",
        "            labels.append(self.classes.index(member.find('name').text))\n",
        "            \n",
        "            # xmin = left corner x-coordinates\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            # xmax = right corner x-coordinates\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            # ymin = left corner y-coordinates\n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            # ymax = right corner y-coordinates\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "            \n",
        "            # resize the bounding boxes according to the...\n",
        "            # ... desired `width`, `height`\n",
        "            xmin_final = (xmin/image_width)*self.width\n",
        "            xmax_final = (xmax/image_width)*self.width\n",
        "            ymin_final = (ymin/image_height)*self.height\n",
        "            yamx_final = (ymax/image_height)*self.height\n",
        "            \n",
        "            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
        "        \n",
        "        # bounding box to tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # area of the bounding boxes\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # no crowd instances\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        # labels to tensor\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        # prepare the final `target` dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "        # apply the image transforms\n",
        "        transform = get_transform()\n",
        "        sample = transform(image = image_target,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "        image_target = sample['image']\n",
        "        target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "        return image_target, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgPJXgyufKJc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7HEKSQLJtPc"
      },
      "outputs": [],
      "source": [
        "dataset = AcneDetectDataset(image_dir, annotation_dir, 512, 512, classes)\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers= 0, collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VZQM9XnZi60"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def create_model(num_classes):\n",
        "    \n",
        "    # load Faster RCNN pre-trained model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    \n",
        "    # get the number of input features \n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # define a new head for the detector with required number of classes\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU23O-ifa_l5"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHPeaXipaAST"
      },
      "outputs": [],
      "source": [
        "def train(train_data_loader, model, optimizer):\n",
        "    print('Training')\n",
        "    \n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "    \n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "        \n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0hhStlTcilu"
      },
      "outputs": [],
      "source": [
        "model = create_model(num_classes= num_classes)\n",
        "model = model.to(DEVICE)\n",
        "#params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = train(train_loader, model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHQZP8e3FRvI"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuSFHoN1M-av"
      },
      "outputs": [],
      "source": [
        " image_paths = glob.glob(f\"{image_dir}/*.jpg\")[:8]\n",
        " image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T_bHfhrOZe9"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r8g5U3s2LROu"
      },
      "outputs": [],
      "source": [
        "detection_threshold = 0.5\n",
        "test_images = image_paths\n",
        "model = model.eval()\n",
        "for i in range(len(test_images)):\n",
        "    # get the image file name for saving output later on\n",
        "    image_name = test_images[i].split('/')[-1].split('.')[0]\n",
        "    image = cv2.imread(test_images[i])\n",
        "    orig_image = image.copy()\n",
        "    # BGR to RGB\n",
        "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    # make the pixel range between 0 and 1\n",
        "    image /= 255.0\n",
        "    # bring color channels to front\n",
        "    image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
        "    # convert to tensor\n",
        "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
        "    # add batch dimension\n",
        "    image = torch.unsqueeze(image, 0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "    \n",
        "    # load all detection to CPU for further operations\n",
        "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
        "    # carry further only if there are detected boxes\n",
        "    if len(outputs[0]['boxes']) != 0:\n",
        "        boxes = outputs[0]['boxes'].data.numpy()\n",
        "        scores = outputs[0]['scores'].data.numpy()\n",
        "        # filter out boxes according to `detection_threshold`\n",
        "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
        "        draw_boxes = boxes.copy()\n",
        "        # get all the predicited class names\n",
        "        pred_classes = classes\n",
        "        \n",
        "        # draw the bounding boxes and write the class name on top of it\n",
        "        for j, box in enumerate(draw_boxes):\n",
        "            cv2.rectangle(orig_image,\n",
        "                        (int(box[0]), int(box[1])),\n",
        "                        (int(box[2]), int(box[3])),\n",
        "                        (0, 0, 255), 2)\n",
        "            cv2.putText(orig_image, 'acne', \n",
        "                        (int(box[0]), int(box[1]-5)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
        "                        2, lineType=cv2.LINE_AA)\n",
        "        cv2_imshow(orig_image)\n",
        "\n",
        "    print(f\"Image {i+1} done...\")\n",
        "    print('-'*50)\n",
        "print('TEST PREDICTIONS COMPLETE')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}